{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11c Bidirectional LSTMs.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNMw0+bZg7do5244cQVAxhO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/timeSeries/blob/main/11c_Bidirectional_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bidirectional LSTMs**"
      ],
      "metadata": {
        "id": "PEAWY-kqnZgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional LSTMs are when we have the LSTM learn the input forwards and backwards, then combine the two outputs. "
      ],
      "metadata": {
        "id": "VPypn9NvnhSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN has the limitation that it processes inputs in strict temporal order. This means current input has context of previous inputs but not the future. Bidirectional RNN (BRNN) duplicates the RNN processing chain so that inputs are processed in both forward and reverse time order. This allows a BRNN to look at future context as well.<br>\n",
        "LSTM does better than RNN in capturing long-term dependencies. Bidirectional LSTM (BiLSTM) in particular is a popular choice in NLP.<br>\n",
        "\n",
        "[Developedia](https://devopedia.org/bidirectional-rnn#:~:text=Bidirectional%20RNN%20(%20BRNN%20)%20duplicates%20the,at%20future%20context%20as%20well.&text=LSTM%20does%20better%20than%20RNN%20in%20capturing%20long%2Dterm%20dependencies.)"
      ],
      "metadata": {
        "id": "XqqN7w2zt9qR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rDDcZyBjjm2"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Bidirectional\n",
        "from keras.utils.vis_utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequence)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the sequence\n",
        "    if end_ix > len(sequence)-1:\n",
        "     break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return array(X), array(y)\n"
      ],
      "metadata": {
        "id": "Mrxu9iRNuaJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]"
      ],
      "metadata": {
        "id": "z0-JNkosuf2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1"
      ],
      "metadata": {
        "id": "BJOeSn9FuioB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "# define model"
      ],
      "metadata": {
        "id": "Cxi5gUY7ueEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# fit model\n",
        "model.fit(X, y, epochs=200, verbose=0)"
      ],
      "metadata": {
        "id": "CAOvnxKduozS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "SPldki49vEnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a prediction:<br>\n",
        "We are expecting 120"
      ],
      "metadata": {
        "id": "rKlFoQsAuy3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = array([90,100,110])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "id": "Tm2RPJ5Uul02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}