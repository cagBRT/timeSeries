{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11b Stacked LSTMs.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOOajuPnUeAxWGTMeXX2Ok5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/timeSeries/blob/main/11b_Stacked_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stacked LSTMs**"
      ],
      "metadata": {
        "id": "4gl-AFpufcny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked LSTM has multiple hidden LSTM layers. <br>\n",
        "The reason to stack LSTMs is because deep models work better on some tasks, as compared to shallow models. <br>\n",
        "There is not a lot of research to prove deep LSTMs work better than shallow ones, but there are examples of this being the case. "
      ],
      "metadata": {
        "id": "tgic87jqfyv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence. We can address this by having the LSTM output a value for each time step in the input data by setting the return sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next. "
      ],
      "metadata": {
        "id": "Zo6MqFAHhl0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "zwf0k40yiDJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequence)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the sequence\n",
        "    if end_ix > len(sequence)-1:\n",
        "      break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return array(X), array(y)"
      ],
      "metadata": {
        "id": "C_GYTL1miJod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]"
      ],
      "metadata": {
        "id": "AzeO5QdMiRV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 3\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)"
      ],
      "metadata": {
        "id": "acxc80ZOiXGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))"
      ],
      "metadata": {
        "id": "FLJpT6kLiZzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Stacked LSTM"
      ],
      "metadata": {
        "id": "E_G7ZOmyhtya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8ZMU0lAdouC"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps,\n",
        "n_features)))\n",
        "model.add(LSTM(50, activation='relu')) \n",
        "model.add(Dense(1)) \n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make a prediction**<br>\n",
        "We are expection 100"
      ],
      "metadata": {
        "id": "kE5pS1FdipAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "# demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps, n_features)) \n",
        "yhat = model.predict(x_input, verbose=0) \n",
        "print(yhat)"
      ],
      "metadata": {
        "id": "NQG599wUid4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**<br>\n",
        "Increase the number of LSTM layers and then predict out to further time steps. \n",
        "How does the model behave?"
      ],
      "metadata": {
        "id": "JOVwSSd1i8QN"
      }
    }
  ]
}